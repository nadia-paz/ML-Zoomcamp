{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "\n",
    "import wrangle as wr\n",
    "import regression as regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce chances of getting a Singular matrix, we can use regularization: multiply the diagonal matrix by some number, normally `zero point value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = '../data/cars.csv'\n",
    "df = pd.read_csv(data)\n",
    "df = wr.rename_columns(df)\n",
    "df_train, df_val, df_test = wr.split_data(df)\n",
    "y_train, y_val, y_test = wr.get_target_vars(df_train, df_val, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  preparation function below will return a singular matrix (cell 24 in `03_categorical_vars.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'popularity']\n",
    "def prepare_X(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    df: dataframe for the baseline model\n",
    "    cols: numeric column names\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    features = base.copy()\n",
    "\n",
    "    df['age'] = 2017 - df.year\n",
    "    features.append('age')\n",
    "    \n",
    "    # got through top-5 car brands\n",
    "    for v in list(df.make.value_counts().head(5).index):\n",
    "        feature = 'is_make_%s' % v\n",
    "        df[feature] = (df['make'] == v).astype(int)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # go through number of doors\n",
    "    for v in [2, 3, 4]:\n",
    "        feature = 'num_doors_%s' % v\n",
    "        df[feature] = (df['number_of_doors'] == v).astype(int)\n",
    "        features.append(feature)\n",
    "\n",
    "    # top-3 transmission\n",
    "    for v in df.transmission_type.value_counts().head(3):\n",
    "        feature = 'transmission_%s' % v \n",
    "        df[feature] = (df.transmission_type == v).astype('uint8')\n",
    "        features.append(feature)\n",
    "\n",
    "    # top-4 engine fuel type\n",
    "    for v in df.engine_fuel_type.value_counts().head(4):\n",
    "        feature = 'engine_fuel_%s' % v \n",
    "        df[feature] = (df.engine_fuel_type == v).astype('uint8')\n",
    "        features.append(feature)\n",
    "\n",
    "    df_new = df[features]\n",
    "    df_new = df_new.fillna(0)\n",
    "    X = df_new.values\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/nadina/Documents/Zoomcapm/ML-Zoomcamp/week2/04_regularization.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadina/Documents/Zoomcapm/ML-Zoomcamp/week2/04_regularization.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# copy paste train model/calculate score\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadina/Documents/Zoomcapm/ML-Zoomcamp/week2/04_regularization.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_train \u001b[39m=\u001b[39m prepare_X(df_train)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nadina/Documents/Zoomcapm/ML-Zoomcamp/week2/04_regularization.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m w0, w \u001b[39m=\u001b[39m regr\u001b[39m.\u001b[39;49mtrain_linear_regression(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadina/Documents/Zoomcapm/ML-Zoomcamp/week2/04_regularization.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m y_pred \u001b[39m=\u001b[39m w0 \u001b[39m+\u001b[39m X_train\u001b[39m.\u001b[39mdot(w)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadina/Documents/Zoomcapm/ML-Zoomcamp/week2/04_regularization.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, rmse(y_train, y_pred))\n",
      "File \u001b[0;32m~/Documents/Zoomcapm/ML-Zoomcamp/week2/regression.py:20\u001b[0m, in \u001b[0;36mtrain_linear_regression\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m     18\u001b[0m XTX \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mdot(X)\n\u001b[1;32m     19\u001b[0m \u001b[39m# inverse XTX\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m XTX_inv \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49minv(XTX)\n\u001b[1;32m     21\u001b[0m \u001b[39m# calculate weights\u001b[39;00m\n\u001b[1;32m     22\u001b[0m w \u001b[39m=\u001b[39m XTX_inv\u001b[39m.\u001b[39mdot(X\u001b[39m.\u001b[39mT)\u001b[39m.\u001b[39mdot(y_train)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/Caskroom/mambaforge/base/lib/python3.10/site-packages/numpy/linalg/linalg.py:538\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    536\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->D\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->d\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    537\u001b[0m extobj \u001b[39m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 538\u001b[0m ainv \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39;49minv(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[1;32m    539\u001b[0m \u001b[39mreturn\u001b[39;00m wrap(ainv\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m/usr/local/Caskroom/mambaforge/base/lib/python3.10/site-packages/numpy/linalg/linalg.py:89\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSingular matrix\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# copy paste train model/calculate score from cell 25 of categrical vars file\n",
    "X_train = prepare_X(df_train)\n",
    "w0, w = regr.train_linear_regression(X_train, y_train)\n",
    "\n",
    "y_pred = w0 + X_train.dot(w)\n",
    "print('train', regr.rmse(y_train, y_pred))\n",
    "\n",
    "X_val = prepare_X(df_val)\n",
    "y_pred_val = w0 + X_val.dot(w)\n",
    "print('validation', regr.rmse(y_val, y_pred_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression_reg(X_train: np.array, y_train: np.array, r:int=0.01):\n",
    "    # fit on train\n",
    "    # added regularization\n",
    "    '''\n",
    "    Parameters:\n",
    "        X_train: 2-D array of features\n",
    "        y_train: 1-D array of target variable\n",
    "    The function calculates weights for linear regression equation.\n",
    "    Returns:\n",
    "        w[0] -> float, bias (y-intersect)\n",
    "        w[1:] -> array of weights (floats)\n",
    "    '''\n",
    "    # add 1 to the beginning of every vector in features\n",
    "    X = np.insert(X_train, 0, np.ones(len(X_train)), axis = 1)\n",
    "    # get gram matrix\n",
    "    XTX = X.T.dot(X)\n",
    "    # regularization\n",
    "    reg = r * np.eye(XTX.shape[0])\n",
    "    XTX = XTX + reg\n",
    "    # inverse XTX\n",
    "    XTX_inv = np.linalg.inv(XTX)\n",
    "    # calculate weights\n",
    "    w = XTX_inv.dot(X.T).dot(y_train)\n",
    "    bias = w[0]\n",
    "    weights = w[1:]\n",
    "\n",
    "    return bias, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.5026765950096316\n",
      "validation 0.5076003111192242\n"
     ]
    }
   ],
   "source": [
    "# same as with an error, but with regularization\n",
    "X_train = prepare_X(df_train)\n",
    "w0, w = train_linear_regression_reg(X_train, y_train)\n",
    "\n",
    "y_pred = w0 + X_train.dot(w)\n",
    "print('train', regr.rmse(y_train, y_pred))\n",
    "\n",
    "X_val = prepare_X(df_val)\n",
    "y_pred_val = w0 + X_val.dot(w)\n",
    "print('validation', regr.rmse(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization helps to improve the score. In the last model of the previous file RMSE score was way too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy / paste from 03_categorical_vars.ipynb\n",
    "\n",
    "base = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'popularity']\n",
    "def prepare_X(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    df: dataframe for the baseline model\n",
    "    cols: numeric column names\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    features = base.copy()\n",
    "\n",
    "    df['age'] = 2017 - df.year\n",
    "    features.append('age')\n",
    "    \n",
    "    # go through number of doors\n",
    "    for v in [2, 3, 4]:\n",
    "        feature = 'num_doors_%s' % v\n",
    "        df[feature] = (df['number_of_doors'] == v).astype('uint8')\n",
    "        features.append(feature)\n",
    "\n",
    "    categorical_vars = ['make',  'engine_fuel_type', 'transmission_type', 'driven_wheels', 'market_category', 'vehicle_size', 'vehicle_style']\n",
    "    categories = {}\n",
    "\n",
    "    for c in categorical_vars:\n",
    "        categories[c] = list(df[c].value_counts().head().index)\n",
    "\n",
    "    for c, values in categories.items():\n",
    "        for v in values:\n",
    "            df['%s_%s' % (c, v)] = (df[c] == v).astype('uint8')\n",
    "            features.append('%s_%s' % (c, v))\n",
    "\n",
    "\n",
    "    df_new = df[features]\n",
    "    df_new = df_new.fillna(0)\n",
    "    X = df_new.values\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 912.5690053266312\n",
      "validation 1427.9090319413954\n"
     ]
    }
   ],
   "source": [
    "# cope paste train model/calculate score\n",
    "X_train = prepare_X(df_train)\n",
    "w0, w = regr.train_linear_regression(X_train, y_train)\n",
    "\n",
    "y_pred = w0 + X_train.dot(w)\n",
    "print('train', regr.rmse(y_train, y_pred))\n",
    "\n",
    "X_val = prepare_X(df_val)\n",
    "y_pred_val = w0 + X_val.dot(w)\n",
    "print('validation', regr.rmse(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.45664176729583617\n",
      "validation 0.45935602351429405\n"
     ]
    }
   ],
   "source": [
    "# cope paste train model/calculate score\n",
    "X_train = prepare_X(df_train)\n",
    "w0, w = train_linear_regression_reg(X_train, y_train)\n",
    "\n",
    "y_pred = w0 + X_train.dot(w)\n",
    "print('train', regr.rmse(y_train, y_pred))\n",
    "\n",
    "X_val = prepare_X(df_val)\n",
    "y_pred_val = w0 + X_val.dot(w)\n",
    "print('validation', regr.rmse(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE score significally improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 -8.271740080240131e+16 1427.9090319413954\n",
      "1e-05 2.9492300146084087 0.4593257840345944\n",
      "0.0001 6.238283384441287 0.45932602468209155\n",
      "0.001 6.295212133272979 0.45932859929368697\n",
      "0.01 6.2727424567851795 0.45935602351429405\n",
      "0.1 6.078293836119147 0.4597472961194152\n",
      "1 5.3187316820049295 0.46388171086208285\n",
      "10 4.189946611709698 0.4815516830146857\n"
     ]
    }
   ],
   "source": [
    "for r in [0.0, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]:\n",
    "    X_train = prepare_X(df_train)\n",
    "    w0, w = train_linear_regression_reg(X_train, y_train, r = r)\n",
    "\n",
    "    X_val = prepare_X(df_val)\n",
    "    y_pred_val = w0 + X_val.dot(w)\n",
    "    score = regr.rmse(y_val, y_pred_val)\n",
    "\n",
    "    print(r, w0, score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1e-06 0.4593257343441555\n",
      "0.0001 0.45932602468209155\n",
      " 0.001 0.45932859929368697\n",
      "  0.01 0.45935602351429405\n",
      "   0.1 0.4597472961194152\n",
      "     1 0.46388171086208285\n",
      "     5 0.4727830448928472\n",
      "    10 0.4815516830146857\n"
     ]
    }
   ],
   "source": [
    "X_train = prepare_X(df_train)\n",
    "X_val = prepare_X(df_val)\n",
    "\n",
    "for r in [0.000001, 0.0001, 0.001, 0.01, 0.1, 1, 5, 10]:\n",
    "    w_0, w = train_linear_regression_reg(X_train, y_train, r=r)\n",
    "    y_pred = w_0 + X_val.dot(w)\n",
    "    print('%6s' %r, regr.rmse(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: 0.4593257343441555\n",
      "test: 0.47395909187296353\n"
     ]
    }
   ],
   "source": [
    "X_train = prepare_X(df_train)\n",
    "w_0, w = train_linear_regression_reg(X_train, y_train, r=0.000001)\n",
    "\n",
    "X_val = prepare_X(df_val)\n",
    "y_pred = w_0 + X_val.dot(w)\n",
    "print('validation:', regr.rmse(y_val, y_pred))\n",
    "\n",
    "X_test = prepare_X(df_test)\n",
    "y_pred = w_0 + X_test.dot(w)\n",
    "print('test:', regr.rmse(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
